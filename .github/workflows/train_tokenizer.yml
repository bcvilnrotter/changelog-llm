name: Train Tokenizer

on:
  workflow_dispatch:  # Manual trigger only

env:
  PYTHON_VERSION: '3.12'

jobs:
  train-tokenizer:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
      with:
        ref: ${{ github.ref }}
        fetch-depth: 0  # Full history for changelog

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Check database
      run: |
        # Check if the database file exists
        if [ -f "data/changelog.db" ]; then
          echo "Database file exists"
          
          # Install sqlite3 command-line tool
          sudo apt-get update
          sudo apt-get install -y sqlite3
          
          # Check database schema
          echo "Database schema:"
          sqlite3 data/changelog.db ".schema"
          
          # Count entries in the database
          echo "Number of entries in the database:"
          sqlite3 data/changelog.db "SELECT COUNT(*) FROM entries;"
          
          # Check if there are any main pages
          echo "Number of main pages in the database:"
          sqlite3 data/changelog.db "SELECT COUNT(*) FROM entries WHERE is_revision = 0;"
        else
          echo "Database file does not exist"
        fi

    - name: Configure Git
      run: |
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"

    - name: Extract page titles from database
      run: |
        python scripts/extract_titles.py --output titles.json --debug
        
        # Display the contents of the titles.json file
        echo "Contents of titles.json:"
        cat titles.json || echo "titles.json not found or empty"
        
    - name: Fetch Wikipedia pages
      run: |
        # Create data/raw directory if it doesn't exist
        mkdir -p data/raw
        
        # Fetch Wikipedia pages using titles from the database
        python scripts/fetch_wikipedia.py --titles "$(cat titles.json)" --debug
        
    - name: Train tokenizer
      run: |
        python scripts/train_tokenizer.py --vocab-size 10000 --debug

    - name: Push tokenizer to repository
      run: |
        # Ensure we're on the latest commit
        git checkout main
        git pull origin main
    
        # Stage tokenizer files
        git add models/tokenizer/
    
        # Create commit with tokenizer details
        git commit -m "Tokenizer training: $(date +'%Y-%m-%d')
    
        - Trained on all available data
        - Vocabulary size: 10000"
    
        # Push the changes
        git push origin main

    - name: Upload tokenizer artifacts
      uses: actions/upload-artifact@v4
      with:
        name: tokenizer
        path: models/tokenizer/
        retention-days: 30  # Keep for a month

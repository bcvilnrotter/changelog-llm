name: Train Tokenizer

on:
  workflow_dispatch:  # Manual trigger only

env:
  PYTHON_VERSION: '3.12'

jobs:
  train-tokenizer:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
      with:
        ref: ${{ github.ref }}
        fetch-depth: 0  # Full history for changelog

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Check database
      run: |
        # Install sqlite3 command-line tool
        sudo apt-get update
        sudo apt-get install -y sqlite3
        
        # Check if any database shards exist
        SHARDS=$(find data -name "changelog_*.db" | wc -l)
        
        if [ "$SHARDS" -gt 0 ]; then
          echo "Found $SHARDS database shards"
          
          # List all shards
          echo "Database shards:"
          find data -name "changelog_*.db" -exec ls -lh {} \;
          
          # Check database schema (using the first shard)
          FIRST_SHARD=$(find data -name "changelog_*.db" | head -n 1)
          echo "Database schema (from $FIRST_SHARD):"
          sqlite3 "$FIRST_SHARD" ".schema"
          
          # Count entries in all shards
          echo "Number of entries in all shards:"
          TOTAL=0
          for SHARD in $(find data -name "changelog_*.db"); do
            COUNT=$(sqlite3 "$SHARD" "SELECT COUNT(*) FROM entries;")
            echo "  $SHARD: $COUNT entries"
            TOTAL=$((TOTAL + COUNT))
          done
          echo "Total entries across all shards: $TOTAL"
          
          # Check if there are any main pages
          echo "Number of main pages in all shards:"
          MAIN_TOTAL=0
          for SHARD in $(find data -name "changelog_*.db"); do
            COUNT=$(sqlite3 "$SHARD" "SELECT COUNT(*) FROM entries WHERE is_revision = 0;")
            echo "  $SHARD: $COUNT main pages"
            MAIN_TOTAL=$((MAIN_TOTAL + COUNT))
          done
          echo "Total main pages across all shards: $MAIN_TOTAL"
          
          # Check shard index
          if [ -f "data/shard_index.json" ]; then
            echo "Shard index exists"
            echo "Shard index size: $(du -h data/shard_index.json | cut -f1)"
            echo "Number of entries in shard index: $(grep -o "\"page_id\"" data/shard_index.json | wc -l)"
          else
            echo "Shard index does not exist"
          fi
        elif [ -f "data/changelog.db" ]; then
          echo "Legacy database file exists (not sharded)"
          
          # Check database schema
          echo "Database schema:"
          sqlite3 data/changelog.db ".schema"
          
          # Count entries in the database
          echo "Number of entries in the database:"
          sqlite3 data/changelog.db "SELECT COUNT(*) FROM entries;"
          
          # Check if there are any main pages
          echo "Number of main pages in the database:"
          sqlite3 data/changelog.db "SELECT COUNT(*) FROM entries WHERE is_revision = 0;"
          
          echo "WARNING: Database is not sharded. Consider migrating to sharded database."
        else
          echo "No database files exist"
        fi

    - name: Configure Git
      run: |
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"

    - name: Extract page titles from database
      run: |
        python scripts/extract_titles.py --output titles.json --debug
        
        # Display the contents of the titles.json file
        echo "Contents of titles.json:"
        cat titles.json || echo "titles.json not found or empty"
        
    - name: Fetch Wikipedia pages
      run: |
        # Create data/raw directory if it doesn't exist
        mkdir -p data/raw
        
        # Fetch Wikipedia pages using titles from the database
        python scripts/fetch_wikipedia.py --titles "$(cat titles.json)" --debug
        
    - name: Train tokenizer
      run: |
        python scripts/train_tokenizer.py --vocab-size 10000 --debug

    - name: Push tokenizer to repository
      run: |
        # Ensure we're on the latest commit
        git checkout main
        git pull origin main
    
        # Stage tokenizer files
        git add models/tokenizer/
    
        # Create commit with tokenizer details
        git commit -m "Tokenizer training: $(date +'%Y-%m-%d')
    
        - Trained on all available data
        - Vocabulary size: 10000"
    
        # Push the changes
        git push origin main

    - name: Upload tokenizer artifacts
      uses: actions/upload-artifact@v4
      with:
        name: tokenizer
        path: models/tokenizer/
        retention-days: 30  # Keep for a month

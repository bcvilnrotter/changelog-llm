name: Train Tokenizer

on:
  workflow_dispatch:  # Manual trigger only

env:
  PYTHON_VERSION: '3.12'

jobs:
  train-tokenizer:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
      with:
        ref: ${{ github.ref }}
        fetch-depth: 0  # Full history for changelog

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Configure Git
      run: |
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"

    - name: Extract page titles from database
      run: |
        python scripts/extract_titles.py --output titles.json
        
    - name: Fetch Wikipedia pages
      run: |
        # Create data/raw directory if it doesn't exist
        mkdir -p data/raw
        
        # Fetch Wikipedia pages using titles from the database
        python scripts/fetch_wikipedia.py --titles "$(cat titles.json)"
        
    - name: Train tokenizer
      run: |
        python scripts/train_tokenizer.py --vocab-size 10000

    - name: Push tokenizer to repository
      run: |
        # Ensure we're on the latest commit
        git checkout main
        git pull origin main
    
        # Stage tokenizer files
        git add models/tokenizer/
    
        # Create commit with tokenizer details
        git commit -m "Tokenizer training: $(date +'%Y-%m-%d')
    
        - Trained on all available data
        - Vocabulary size: 10000"
    
        # Push the changes
        git push origin main

    - name: Upload tokenizer artifacts
      uses: actions/upload-artifact@v4
      with:
        name: tokenizer
        path: models/tokenizer/
        retention-days: 30  # Keep for a month

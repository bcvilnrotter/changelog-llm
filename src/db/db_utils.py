"""
Database utility functions for the changelog-llm project.
This module provides common database operations for the training process.
"""

import json
import datetime
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
import sqlite3

from src.db.db_schema import get_db_connection, init_db
from src.db.shard_manager import get_shard_manager

logger = logging.getLogger(__name__)

def create_training_run(model_name: str, base_model: str, hyperparameters: Dict, git_commit: Optional[str] = None) -> int:
    """
    Create a new training run entry in the database.
    
    Args:
        model_name (str): Name of the model being trained
        base_model (str): Name of the base model being fine-tuned
        hyperparameters (dict): Training hyperparameters
        git_commit (str, optional): Git commit hash
    
    Returns:
        int: ID of the new training run
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    timestamp = datetime.datetime.utcnow().isoformat() + "Z"
    
    cursor.execute('''
        INSERT INTO training_runs (
            model_name, base_model, hyperparameters, git_commit, 
            status, timestamp, metrics
        ) VALUES (?, ?, ?, ?, ?, ?, ?)
    ''', (
        model_name, 
        base_model, 
        json.dumps(hyperparameters), 
        git_commit, 
        'running', 
        timestamp, 
        '{}'
    ))
    
    run_id = cursor.lastrowid
    conn.commit()
    conn.close()
    
    if run_id is None:
        return -1  # Return a sentinel value if no ID was generated
    return run_id

def update_training_run_status(run_id: int, status: str, metrics: Optional[Dict] = None) -> bool:
    """
    Update the status and metrics of a training run.
    
    Args:
        run_id (int): ID of the training run
        status (str): New status ('running', 'completed', 'failed')
        metrics (dict, optional): Training metrics to save
    
    Returns:
        bool: True if update was successful
    """
    if status not in ['running', 'completed', 'failed']:
        raise ValueError("Status must be one of: running, completed, failed")
    
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # Get existing metrics
    cursor.execute('SELECT metrics FROM training_runs WHERE id = ?', (run_id,))
    result = cursor.fetchone()
    
    if not result:
        conn.close()
        return False
    
    # Update metrics
    existing_metrics = json.loads(result['metrics'])
    if metrics:
        existing_metrics.update(metrics)
    
    # Update record
    cursor.execute('''
        UPDATE training_runs 
        SET status = ?, metrics = ?
        WHERE id = ?
    ''', (status, json.dumps(existing_metrics), run_id))
    
    success = cursor.rowcount > 0
    conn.commit()
    conn.close()
    
    return success

def add_training_examples(run_id: int, examples: List[Dict]) -> int:
    """
    Add training examples to a training run.
    
    Args:
        run_id (int): ID of the training run
        examples (list): List of example dictionaries with 'input', 'target', and optional 'type' and 'metadata'
    
    Returns:
        int: Number of examples added
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # Prepare insertion
    count = 0
    for example in examples:
        if 'input' not in example or 'target' not in example:
            continue
        
        example_type = example.get('type', 'general')
        metadata = json.dumps(example.get('metadata', {}))
        
        cursor.execute('''
            INSERT INTO training_examples (
                run_id, input_text, target_text, example_type, metadata
            ) VALUES (?, ?, ?, ?, ?)
        ''', (run_id, example['input'], example['target'], example_type, metadata))
        
        count += 1
    
    conn.commit()
    conn.close()
    
    return count

def add_model_output(run_id: int, input_text: str, output_text: str, metadata: Optional[Dict] = None) -> int:
    """
    Add a model output to a training run.
    
    Args:
        run_id (int): ID of the training run
        input_text (str): Input prompt provided to the model
        output_text (str): Output generated by the model
        metadata (dict, optional): Additional metadata about the output
    
    Returns:
        int: ID of the new output entry
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    timestamp = datetime.datetime.utcnow().isoformat() + "Z"
    metadata_json = json.dumps(metadata or {})
    
    cursor.execute('''
        INSERT INTO model_outputs (
            run_id, input_text, output_text, timestamp, metadata
        ) VALUES (?, ?, ?, ?, ?)
    ''', (run_id, input_text, output_text, timestamp, metadata_json))
    
    output_id = cursor.lastrowid
    conn.commit()
    conn.close()
    
    if output_id is None:
        return -1  # Return a sentinel value if no ID was generated
    return output_id

def get_training_run(run_id: int) -> Optional[Dict]:
    """
    Get details of a specific training run.
    
    Args:
        run_id (int): ID of the training run
    
    Returns:
        dict: Training run details or None if not found
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT * FROM training_runs WHERE id = ?
    ''', (run_id,))
    
    result = cursor.fetchone()
    conn.close()
    
    if not result:
        return None
    
    # Convert to dictionary with parsed JSON fields
    run = dict(result)
    run['hyperparameters'] = json.loads(run['hyperparameters'])
    run['metrics'] = json.loads(run['metrics'])
    
    return run

def get_all_training_runs() -> List[Dict]:
    """
    Get a list of all training runs.
    
    Returns:
        list: List of training run summaries
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT id, model_name, base_model, status, timestamp 
        FROM training_runs
        ORDER BY timestamp DESC
    ''')
    
    runs = [dict(row) for row in cursor.fetchall()]
    conn.close()
    
    return runs

def get_training_examples(run_id: Optional[int] = None, example_type: Optional[str] = None, limit: Optional[int] = None) -> List[Dict]:
    """
    Get training examples, optionally filtered by run_id and type.
    
    Args:
        run_id (int, optional): Filter by training run ID
        example_type (str, optional): Filter by example type
        limit (int, optional): Limit the number of examples returned
    
    Returns:
        list: List of training examples
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    query = 'SELECT * FROM training_examples'
    params = []
    
    # Build WHERE clause based on filters
    where_clauses = []
    if run_id is not None:
        where_clauses.append('run_id = ?')
        params.append(run_id)
    
    if example_type is not None:
        where_clauses.append('example_type = ?')
        params.append(example_type)
    
    if where_clauses:
        query += ' WHERE ' + ' AND '.join(where_clauses)
    
    query += ' ORDER BY id'
    
    if limit is not None:
        query += ' LIMIT ?'
        params.append(limit)
    
    cursor.execute(query, params)
    
    # Convert to list of dictionaries with parsed metadata
    examples = []
    for row in cursor.fetchall():
        example = dict(row)
        example['metadata'] = json.loads(example['metadata'])
        examples.append(example)
    
    conn.close()
    
    return examples

def log_page(title: str, page_id: str, revision_id: str, content_hash: str, action: str = "added", 
             is_revision: bool = False, parent_id: Optional[str] = None, 
             revision_number: Optional[int] = None) -> int:
    """
    Log a Wikipedia page entry in the database.
    
    Args:
        title (str): Page title
        page_id (str): Wikipedia page ID
        revision_id (str): Wikipedia revision ID
        content_hash (str): Hash of page content
        action (str, optional): Operation type (added/updated/removed)
        is_revision (bool, optional): Whether this entry is a revision of another page
        parent_id (str, optional): ID of the parent page if this is a revision
        revision_number (int, optional): Revision number if this is a revision
    
    Returns:
        int: ID of the inserted entry
    """
    if action not in ["added", "updated", "removed"]:
        raise ValueError("Action must be one of: added, updated, removed")
    
    # Get the shard manager
    shard_manager = get_shard_manager()
    
    # For writing, use the current shard
    db_path = shard_manager.get_shard_for_writing()
    conn = get_db_connection(db_path, for_writing=True)
    cursor = conn.cursor()
    
    timestamp = datetime.datetime.utcnow().isoformat() + "Z"
    
    try:
        cursor.execute('''
            INSERT INTO entries (
                title, page_id, revision_id, timestamp, content_hash,
                action, is_revision, parent_id, revision_number
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            title, page_id, revision_id, timestamp, content_hash,
            action, is_revision, parent_id, revision_number
        ))
        
        entry_id = cursor.lastrowid
        if entry_id is None:
            conn.rollback()
            return -1  # Return a sentinel value if no ID was generated
        
        # Create initial training metadata record
        cursor.execute('''
            INSERT INTO training_metadata (entry_id, used_in_training)
            VALUES (?, 0)
        ''', (entry_id,))
        
        conn.commit()
        
        # Update the shard index
        shard_manager.shard_index.add_page(page_id, db_path)
        
        return entry_id
    
    except sqlite3.IntegrityError:
        # Handle case where page_id already exists
        if action == "updated":
            # Update existing entry
            cursor.execute('''
                UPDATE entries
                SET revision_id = ?, timestamp = ?, content_hash = ?, action = ?
                WHERE page_id = ?
            ''', (revision_id, timestamp, content_hash, action, page_id))
            
            # Get updated entry_id
            cursor.execute('SELECT id FROM entries WHERE page_id = ?', (page_id,))
            entry_id = cursor.fetchone()['id']
            conn.commit()
            return entry_id
        else:
            # For other actions, just return existing entry ID
            cursor.execute('SELECT id FROM entries WHERE page_id = ?', (page_id,))
            entry_id = cursor.fetchone()['id']
            return entry_id
    finally:
        conn.close()

def mark_used_in_training(page_ids: List[str], model_checkpoint: str, 
                          training_metrics: Optional[Dict[str, Dict[str, Any]]] = None) -> None:
    """
    Mark pages as used in training with associated model checkpoint and metrics.
    
    Args:
        page_ids (List[str]): List of page IDs used in training
        model_checkpoint (str): Hash or identifier of the model checkpoint
        training_metrics (Dict, optional): Dictionary mapping page_ids to their training metrics
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    timestamp = datetime.datetime.utcnow().isoformat() + "Z"
    
    for page_id in page_ids:
        # Get entry ID for this page
        cursor.execute('SELECT id FROM entries WHERE page_id = ?', (page_id,))
        entry_row = cursor.fetchone()
        
        if not entry_row:
            continue
        
        entry_id = entry_row['id']
        
        # Get the training metadata ID
        cursor.execute('SELECT id FROM training_metadata WHERE entry_id = ?', (entry_id,))
        metadata_row = cursor.fetchone()
        
        if not metadata_row:
            # Create a new metadata record if none exists
            cursor.execute('''
                INSERT INTO training_metadata (
                    entry_id, used_in_training, training_timestamp, model_checkpoint
                ) VALUES (?, 1, ?, ?)
            ''', (entry_id, timestamp, model_checkpoint))
            metadata_id = cursor.lastrowid
            if metadata_id is None:
                # If we couldn't get an ID, use a default (this shouldn't happen)
                conn.rollback()
                continue
        else:
            metadata_id = metadata_row['id']
            # Update existing metadata
            cursor.execute('''
                UPDATE training_metadata
                SET used_in_training = 1, training_timestamp = ?, model_checkpoint = ?
                WHERE id = ?
            ''', (timestamp, model_checkpoint, metadata_id))
        
        # Add metrics if provided
        if training_metrics and page_id in training_metrics:
            page_metrics = training_metrics[page_id]
            
            # Update loss metrics
            if 'average_loss' in page_metrics and 'relative_loss' in page_metrics:
                # Get loss values with default of 0.0 if not present
                avg_loss = page_metrics.get('average_loss')
                rel_loss = page_metrics.get('relative_loss')
                
                # Convert to float with safe defaults
                avg_loss_float = 0.0 if avg_loss is None else float(avg_loss)
                rel_loss_float = 0.0 if rel_loss is None else float(rel_loss)
                
                cursor.execute('''
                    UPDATE training_metadata
                    SET average_loss = ?, relative_loss = ?
                    WHERE id = ?
                ''', (
                    avg_loss_float,
                    rel_loss_float,
                    metadata_id
                ))
            
            # Handle token impact data with improved error handling and logging
            token_impact = page_metrics.get('token_impact')
            if token_impact and isinstance(token_impact, dict):
                try:
                    # Log token impact data for debugging
                    import logging
                    logger = logging.getLogger(__name__)
                    logger.info(f"Processing token impact data for page_id={page_id}")
                    print(f"DEBUG: Processing token impact data for page_id={page_id}")
                    print(f"DEBUG: metadata_id={metadata_id}, total_tokens={token_impact.get('total_tokens', 0)}")
                    
                    # Create token impact record in token_impacts table (plural)
                    cursor.execute('''
                        INSERT INTO token_impacts (metadata_id, total_tokens)
                        VALUES (?, ?)
                    ''', (metadata_id, token_impact.get('total_tokens', 0)))
                    
                    token_impact_id = cursor.lastrowid
                    if token_impact_id is None:
                        logger.error(f"Failed to get lastrowid for token_impacts insert")
                        print(f"ERROR: Failed to get lastrowid for token_impacts insert")
                        continue
                    
                    logger.info(f"Created token_impacts record with id={token_impact_id}")
                    print(f"DEBUG: Created token_impacts record with id={token_impact_id}")
                    
                    # Add top tokens
                    top_tokens = token_impact.get('top_tokens', [])
                    logger.info(f"Found {len(top_tokens)} top tokens to process")
                    print(f"DEBUG: Found {len(top_tokens)} top tokens to process")
                    
                    for token in top_tokens:
                        try:
                            position = token['position']
                            # Handle context data safely
                            context = token.get('context', [position, position])
                            if not isinstance(context, list) or len(context) < 2:
                                context = [position, position]
                            
                            context_start = context[0]
                            context_end = context[1]
                            
                            # Ensure token_id is an integer
                            token_id = int(token['token_id'])
                            
                            # Insert into top_tokens table
                            cursor.execute('''
                                INSERT INTO top_tokens (
                                    token_impact_id, token_id, position, impact,
                                    context_start, context_end
                                ) VALUES (?, ?, ?, ?, ?, ?)
                            ''', (
                                token_impact_id,
                                token_id,
                                position,
                                float(token['impact']),
                                context_start,
                                context_end
                            ))
                            
                        except Exception as e:
                            logger.error(f"Error processing top token: {str(e)}")
                            logger.error(f"Token data: {token}")
                            # Continue with next token
                            continue
                
                except Exception as e:
                    logger.error(f"Error processing token impact data: {str(e)}")
                    logger.error(f"Token impact data: {token_impact}")
                    # Continue with next page
                    continue
    
    conn.commit()
    conn.close()

def get_page_by_id(page_id: str) -> Optional[Dict]:
    """
    Get a single page by its ID.
    
    Args:
        page_id (str): The page ID to look up
        
    Returns:
        Dict or None: The page entry if found
    """
    # Get the shard manager
    shard_manager = get_shard_manager()
    
    # Get the shard(s) that might contain this page
    shard_paths = shard_manager.get_shard_for_reading(page_id)
    
    for shard_path in shard_paths:
        conn = get_db_connection(shard_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT e.*, tm.used_in_training, tm.training_timestamp, 
                   tm.model_checkpoint, tm.average_loss, tm.relative_loss
            FROM entries e
            LEFT JOIN training_metadata tm ON e.id = tm.entry_id
            WHERE e.page_id = ?
        ''', (page_id,))
        
        row = cursor.fetchone()
        conn.close()
        
        if row:
            # Update the shard index if we found the page
            shard_manager.shard_index.add_page(page_id, shard_path)
            return dict(row)
    
    return None

def get_page_history(page_id: str) -> List[Dict]:
    """
    Get all entries for a specific page ID.
    
    Args:
        page_id (str): Wikipedia page ID
        
    Returns:
        List[Dict]: List of page entries
    """
    try:
        # Get the shard manager
        shard_manager = get_shard_manager()
        
        # Get the shard(s) that might contain this page
        shard_paths = shard_manager.get_shard_for_reading(page_id)
        
        entries = []
        for shard_path in shard_paths:
            try:
                conn = get_db_connection(shard_path)
                cursor = conn.cursor()
                
                try:
                    cursor.execute('''
                        SELECT e.*, tm.used_in_training, tm.training_timestamp, 
                               tm.model_checkpoint, tm.average_loss, tm.relative_loss
                        FROM entries e
                        LEFT JOIN training_metadata tm ON e.id = tm.entry_id
                        WHERE e.page_id = ?
                        ORDER BY e.timestamp DESC
                    ''', (page_id,))
                    
                    for row in cursor.fetchall():
                        try:
                            entries.append(dict(row))
                            # Update the shard index if we found the page
                            shard_manager.shard_index.add_page(page_id, shard_path)
                        except Exception as e:
                            logger.error(f"Error converting row to dict: {str(e)}")
                            # Skip this row and continue
                            continue
                except Exception as e:
                    logger.error(f"Error querying database: {str(e)}")
                finally:
                    conn.close()
                
                # If we found entries, we can stop searching
                if entries:
                    break
                    
            except Exception as e:
                logger.error(f"Error processing shard {shard_path}: {str(e)}")
        
        return entries
    except Exception as e:
        logger.error(f"Unexpected error in get_page_history: {str(e)}")
        return []

def check_updates(page_id: str, revision_id: str) -> bool:
    """
    Check if a page needs updating based on revision ID.
    
    Args:
        page_id (str): Wikipedia page ID
        revision_id (str): Current revision ID to check
        
    Returns:
        bool: True if page needs updating, False otherwise
    """
    try:
        logger.info(f"Checking updates for page_id={page_id}, revision_id={revision_id}")
        
        # Get the shard manager
        shard_manager = get_shard_manager()
        
        # Get the shard(s) that might contain this page
        shard_paths = shard_manager.get_shard_for_reading(page_id)
        
        for shard_path in shard_paths:
            try:
                # First check if the page exists at all
                conn = get_db_connection(shard_path)
                cursor = conn.cursor()
                
                # Use a simple count query first to check existence
                cursor.execute('SELECT COUNT(*) FROM entries WHERE page_id = ?', (page_id,))
                count = cursor.fetchone()[0]
                
                if count == 0:
                    # Page doesn't exist in this shard, try the next one
                    conn.close()
                    continue
                
                # If we get here, the page exists in this shard, so get its revision_id
                cursor.execute('''
                    SELECT revision_id 
                    FROM entries 
                    WHERE page_id = ? 
                    ORDER BY timestamp DESC
                    LIMIT 1
                ''', (page_id,))
                
                result = cursor.fetchone()
                logger.info(f"Found existing entry with page_id={page_id} in shard {shard_path}")
                
                if not result:
                    logger.warning(f"Strange: count was {count} but no result found")
                    conn.close()
                    return True
                
                # Get the revision_id as bytes and decode it safely
                db_revision_id_bytes = result[0]
                if isinstance(db_revision_id_bytes, bytes):
                    db_revision_id = db_revision_id_bytes.decode('utf-8', errors='replace')
                else:
                    db_revision_id = str(db_revision_id_bytes)
                
                logger.info(f"Comparing revision IDs: {db_revision_id} vs {revision_id}")
                
                # Update the shard index since we found the page
                shard_manager.shard_index.add_page(page_id, shard_path)
                
                conn.close()
                return db_revision_id != revision_id
                
            except Exception as e:
                logger.error(f"Error querying shard {shard_path}: {str(e)}")
                logger.error(f"Exception type: {type(e).__name__}")
                # Continue to the next shard
                continue
        
        # If we get here, the page wasn't found in any shard
        logger.info(f"No existing entry found for page_id={page_id}, needs adding")
        return True  # No existing entry, needs adding
            
    except Exception as e:
        logger.error(f"Unexpected error in check_updates: {str(e)}")
        logger.error(f"Exception type: {type(e).__name__}")
        # If there's an error, assume we need to add the page
        return True

def get_unused_pages() -> List[Dict]:
    """
    Get all pages that haven't been used in training.
    
    Returns:
        List[Dict]: List of page entries that haven't been used in training
    """
    # Get the shard manager
    shard_manager = get_shard_manager()
    
    # Get all shards
    shard_paths = shard_manager.get_all_shards()
    
    entries = []
    for shard_path in shard_paths:
        try:
            conn = get_db_connection(shard_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT e.*
                FROM entries e
                JOIN training_metadata tm ON e.id = tm.entry_id
                WHERE tm.used_in_training = 0
            ''')
            
            for row in cursor.fetchall():
                entry = dict(row)
                entries.append(entry)
                
                # Update the shard index
                page_id = entry.get('page_id')
                if page_id:
                    shard_manager.shard_index.add_page(page_id, shard_path)
            
            conn.close()
            
        except Exception as e:
            logger.error(f"Error querying shard {shard_path}: {str(e)}")
            # Continue to the next shard
            continue
    
    return entries

def get_page_revisions(page_id: str) -> List[Dict]:
    """
    Get all revision entries for a page.
    
    Args:
        page_id (str): Wikipedia page ID
        
    Returns:
        List[Dict]: List of revision entries for the page, sorted by revision_number
    """
    # Get the shard manager
    shard_manager = get_shard_manager()
    
    # Get the shard(s) that might contain this page
    shard_paths = shard_manager.get_shard_for_reading(page_id)
    
    revisions = []
    for shard_path in shard_paths:
        try:
            conn = get_db_connection(shard_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT e.*, tm.used_in_training, tm.training_timestamp, 
                       tm.model_checkpoint, tm.average_loss, tm.relative_loss
                FROM entries e
                LEFT JOIN training_metadata tm ON e.id = tm.entry_id
                WHERE e.is_revision = 1 AND e.parent_id = ?
                ORDER BY e.revision_number
            ''', (page_id,))
            
            for row in cursor.fetchall():
                revision = dict(row)
                revisions.append(revision)
                
                # Update the shard index
                revision_page_id = revision.get('page_id')
                if revision_page_id:
                    shard_manager.shard_index.add_page(revision_page_id, shard_path)
            
            conn.close()
            
            # If we found revisions, we can stop searching
            if revisions:
                break
                
        except Exception as e:
            logger.error(f"Error querying shard {shard_path}: {str(e)}")
            # Continue to the next shard
            continue
    
    return revisions

def get_main_pages() -> List[Dict]:
    """
    Get all non-revision pages.
    
    Returns:
        List[Dict]: List of main page entries (excluding revisions)
    """
    # Get the shard manager
    shard_manager = get_shard_manager()
    
    # Get all shards
    shard_paths = shard_manager.get_all_shards()
    
    pages = []
    for shard_path in shard_paths:
        try:
            conn = get_db_connection(shard_path)
            cursor = conn.cursor()
            
            # Use a simpler query without the problematic columns
            query = '''
                SELECT e.*, tm.used_in_training
                FROM entries e
                LEFT JOIN training_metadata tm ON e.id = tm.entry_id
                WHERE e.is_revision = 0
            '''
            
            logger.info(f"Executing query on shard {shard_path}: {query}")
            cursor.execute(query)
            
            for row in cursor.fetchall():
                page = dict(row)
                # If training_metadata values are NULL or missing, set defaults
                if page.get("used_in_training") is None:
                    page["used_in_training"] = 0
                
                # Add missing columns with default values
                if "training_timestamp" not in page:
                    page["training_timestamp"] = None
                if "model_checkpoint" not in page:
                    page["model_checkpoint"] = None
                if "average_loss" not in page:
                    page["average_loss"] = None
                if "relative_loss" not in page:
                    page["relative_loss"] = None
                
                pages.append(page)
                
                # Update the shard index
                page_id = page.get('page_id')
                if page_id:
                    shard_manager.shard_index.add_page(page_id, shard_path)
            
            conn.close()
            
        except Exception as e:
            logger.error(f"Error querying shard {shard_path}: {str(e)}")
            # Continue to the next shard
            continue
    
    return pages

def remove_unused_entries() -> int:
    """
    Remove all entries that haven't been used in training.
    
    Returns:
        int: Number of entries removed
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # First, delete related training_metadata entries
        cursor.execute('''
            DELETE FROM training_metadata
            WHERE entry_id IN (
                SELECT e.id
                FROM entries e
                JOIN training_metadata tm ON e.id = tm.entry_id
                WHERE tm.used_in_training = 0
            )
        ''')
        
        # Then delete the entries
        cursor.execute('''
            DELETE FROM entries
            WHERE id IN (
                SELECT e.id
                FROM entries e
                JOIN training_metadata tm ON e.id = tm.entry_id
                WHERE tm.used_in_training = 0
            )
        ''')
        
        count = cursor.rowcount
        conn.commit()
        return count
    except Exception as e:
        import logging
        logger = logging.getLogger(__name__)
        logger.error(f"Error removing unused entries: {str(e)}")
        conn.rollback()
        return 0
    finally:
        conn.close()

def export_to_json(output_path: str = "changelog_export.json") -> bool:
    """
    Export the entire database to a JSON file similar to the original changelog.json format.
    
    Args:
        output_path (str): Path to save the JSON file
        
    Returns:
        bool: True if export was successful
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # Get all entries with their training metadata
    cursor.execute('''
        SELECT e.*, tm.used_in_training, tm.training_timestamp, 
               tm.model_checkpoint, tm.average_loss, tm.relative_loss,
               tm.id as metadata_id
        FROM entries e
        LEFT JOIN training_metadata tm ON e.id = tm.entry_id
    ''')
    
    entries = []
    for row in cursor.fetchall():
        entry = dict(row)
        metadata_id = entry.pop('metadata_id')
        
        # Create training_metadata structure
        training_metadata = {
            "used_in_training": entry.pop('used_in_training'),
            "training_timestamp": entry.pop('training_timestamp'),
            "model_checkpoint": entry.pop('model_checkpoint'),
            "average_loss": entry.pop('average_loss'),
            "relative_loss": entry.pop('relative_loss'),
            "token_impact": None
        }
        
        # Get token impact data if it exists
        if metadata_id is not None:
            cursor.execute('''
                SELECT id, total_tokens
                FROM token_impacts
                WHERE metadata_id = ?
            ''', (metadata_id,))
            
            token_impact_row = cursor.fetchone()
            if token_impact_row:
                token_impact_id = token_impact_row['id']
                total_tokens = token_impact_row['total_tokens']
                
                # Get top tokens
                cursor.execute('''
                    SELECT token_id, position, impact, context_start, context_end
                    FROM top_tokens
                    WHERE token_impact_id = ?
                ''', (token_impact_id,))
                
                top_tokens = []
                for token_row in cursor.fetchall():
                    top_tokens.append({
                        "token_id": token_row['token_id'],
                        "position": token_row['position'],
                        "impact": token_row['impact'],
                        "context": [token_row['context_start'], token_row['context_end']]
                    })
                
                training_metadata["token_impact"] = {
                    "top_tokens": top_tokens,
                    "total_tokens": total_tokens
                }
        
        entry["training_metadata"] = training_metadata
        entries.append(entry)
    
    conn.close()
    
    # Create output JSON
    json_data = {"entries": entries}
    
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        return True
    except Exception:
        return False
